{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tarea 1\n",
        "## Miembros:\n",
        "* Kevin Andrés Marulanda Durán\n",
        "* Samuel Restrepo Aguilar\n",
        "* Yeison Estiben Botero Rivera\n",
        "* Juan Esteban Osorno"
      ],
      "metadata": {
        "id": "Pq3DxU3aquj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##First Method Analysis.\n",
        "\n",
        "The Characteristic Polynomial Method\n",
        "The characteristic polynomial of a matrix A is defined as\n",
        "det(A − λI) = 0,\n",
        "where I is the identity matrix. The (real) roots of this polynomial are the eigenvalues of A. To\n",
        "find the eigenvectors, each eigenvalue λ can be substituted back into (A − λI)x = 0 to solve for\n",
        "the corresponding eigenvector x. This method requires solving a polynomial equation, which can be\n",
        "computationally challenging for large matrices."
      ],
      "metadata": {
        "id": "vEGBwJmVtmMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Theory\n",
        "We ment to solve this problem with the  Leverrier's method, with which we can make an efficient computation to find the coefficents of the characteristic polynomial associated to the matrix.\n",
        "\n",
        "The method works as follows:\n",
        "\n",
        "Let $\\textbf{A}$ be a matrix of nth dimension, the characteristic polynomial is:\n",
        "\n",
        "$p(\\lambda)=\\lambda^n+p_1\\lambda^{n-1}+p_2\\lambda^{n-2}+...+p_{n-1}\\lambda+p_n$\n",
        "\n",
        "The coefficients are computed as follows:\n",
        "\n",
        "\n",
        "$\\begin{aligned}\n",
        "    p_1 &= -s_1 \\\\\n",
        "    p_2 &= \\frac{1}{2} \\left( s_2 + p_1 s_1 \\right) \\\\\n",
        "    &\\vdots \\\\\n",
        "    p_n &= (-1)^{n}\\frac{1}{n} \\left( s_n + p_1 s_{n-1} + \\dots + p_{n-1} s_1 \\right)\n",
        "\\end{aligned}$\n",
        "\n",
        "where the values $s_1,...,s_n$ are the traces of the powers of the square matrix $\\textbf{A}$.\n",
        "\n",
        "$\\begin{aligned}\n",
        "    s_1 &= trace \\textbf{ A} \\\\\n",
        "    s_2 &= trace \\textbf{ A}^2 \\\\\n",
        "    &\\vdots \\\\\n",
        "    s_n &= trace \\textbf{ A}^n\n",
        "\\end{aligned}$\n",
        "\n",
        "where $trace \\textbf{ A}=\\sum_{i=1}^{n}a_{ii}$"
      ],
      "metadata": {
        "id": "9SMpFkTuvXmx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q7Hd20BYAfF",
        "outputId": "2deaab5d-ed58-41e0-c24e-5f8d91cb4455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1,2,3;4,5,6;4,5,6\n",
            "[1, -12.0, -9.0, -0.0]\n",
            "{-0.708, 0.0, 12.708}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import sympy as sp\n",
        "def newton(f, df, p0, delta, epsilon, max1):\n",
        "    for k in range(1, max1 + 1):\n",
        "        p1 = p0 - f(p0) / df(p0)\n",
        "        err = abs(p1 - p0)\n",
        "        relerr = 2 * err / (abs(p1) + delta)\n",
        "        p0 = p1\n",
        "        y = f(p0)\n",
        "\n",
        "        if err < delta or relerr < delta or abs(y) < epsilon:\n",
        "            break\n",
        "\n",
        "    return p0, err, k, y\n",
        "\n",
        "inp = input().split(\";\")\n",
        "list = []\n",
        "for x in inp:\n",
        "  list.append([int(y) for y in x.split(\",\")])\n",
        "lis = []\n",
        "lip = []\n",
        "A = np.array(list, dtype=float)\n",
        "dimA = np.shape(A)[0]\n",
        "I = np.identity(dimA)\n",
        "for x in range(dimA):\n",
        "    I = np.dot(I, A)\n",
        "    lis.append(np.trace(I))\n",
        "lip.append(1)\n",
        "lip.append(-lis[0])\n",
        "for x in range(1, dimA):\n",
        "  acum = 0\n",
        "  for y in range(len(lip)):\n",
        "    acum += lip[y]*lis[len(lip)-(y+1)]\n",
        "  lip.append((-1)*(acum)/(x+1))\n",
        "print(lip)\n",
        "f = np.poly1d(lip)\n",
        "x = sp.Symbol('x')\n",
        "df = sp.diff(f(x),x)\n",
        "df = sp.lambdify(x, df)\n",
        "\n",
        "#Busqueda de las raíces\n",
        "cot = max([abs(x) for x in lip])\n",
        "M = 1+cot\n",
        "m = -1-cot\n",
        "s = set()\n",
        "for x in np.arange(m, M, 1/dimA):\n",
        "  p0, err, k, y = newton(f, df, x, 1e-10, 1e-10, 30)\n",
        "  s.add(round(p0, 3))\n",
        "print(\"Algunos de los valores propios de la matriz son: \",s)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step where we try to find the real roots of the characteristic polynomial\n",
        "of the matrix $\\textbf{A}$ we used the Cauchy criterion for polynomials.\n",
        "\n",
        "The Cauchy criterion states that the set of the real roots of the polynomial $P(z)=p_0z^n+p_1z^{n-1}+\\dots+p_{n-1}z+p_n$ is contained in the set $(-1-\\frac{A}{p_0},1+\\frac{A}{p_0})$, where $A=\\max_{0\\leq{i}\\leq{n}} {|p_{i}|}$.\n",
        "\n",
        "We will prove this fact next:\n",
        "\n",
        "Let $\\bar{z}$ a real root of the polinomyal $P$, we must proof that $\\bar{z}\\in(-1-\\frac{A}{p_0},1+\\frac{A}{p_0})$\n",
        "\n",
        "$\\textbf{Case 1}: |\\bar{z}|=0$\n",
        "\n",
        "This means $\\bar{z}=0$. Let's notice that $0\\in(-1-\\frac{A}{p_0},1+\\frac{A}{p_0})$. Then $\\bar{z}\\in(-1-\\frac{A}{p_0},1+\\frac{A}{p_0})$\n",
        "\n",
        "$\\textbf{Case 2}: |\\bar{z}|>0$\n",
        "\n",
        "We have that:\n",
        "\n",
        "$0=p_0\\bar{z}^n+p_1\\bar{z}^{n-1}+\\cdots+p_{n-1}\\bar{z}+p_n$\n",
        "\n",
        "$p_0\\bar{z}^n=-(p_1\\bar{z}^{n-1}+\\cdots+p_{n-1}\\bar{z}+p_n)$\n",
        "\n",
        "$|p_0\\bar{z}^n|=|p_1\\bar{z}^{n-1}+\\cdots+p_{n-1}\\bar{z}+p_n|$\n",
        "\n",
        "$|p_0||\\bar{z}^n|\\leq|p_1\\bar{z}^{n-1}|+|\\cdots|+|p_{n-1}\\bar{z}|+|p_n|$\n",
        "\n",
        "$\\hspace{1.8cm}=|p_1||\\bar{z}^{n-1}|+|\\cdots|+|p_{n-1}||\\bar{z}|+|p_n|$\n",
        "\n",
        "$\\hspace{1.8cm}\\leq{A|\\bar{z}|^{n-1}+|\\cdots|+A|\\bar{z}|+A}$\n",
        "\n",
        "$\\hspace{1.8cm}={A(|\\bar{z}|^{n-1}+|\\cdots|+|\\bar{z}|+1)}$\n",
        "\n",
        "Now, if $|\\bar{z}|>1$ we got:\n",
        "\n",
        "$(|\\bar{z}|-1)|p_0||\\bar{z}^n|\\leq{A(|\\bar{z}|^{n-1}+|\\cdots|+|\\bar{z}|+1)(|\\bar{z}|-1)}$\n",
        "\n",
        "$(|\\bar{z}|-1)|p_0||\\bar{z}^n|\\leq{A(|\\bar{z}|^n-1)}<{A|\\bar{z}|^n}$\n",
        "\n",
        "$(|\\bar{z}|-1)|p_0|<{A}$\n",
        "\n",
        "$|\\bar{z}|-1<{\\frac{A}{|p_0|}}$\n",
        "\n",
        "$|\\bar{z}|<{1+\\frac{A}{|p_0|}}$\n",
        "\n",
        "Then $\\bar{z}\\in(-1-\\frac{A}{p_0},1+\\frac{A}{p_0})$.\n",
        "\n",
        "If $|\\bar{z}|\\leq{1}$:\n",
        "\n",
        "Let's notice that $[-1,1]\\in(-1-\\frac{A}{|p_0|},1+\\frac{A}{|p_0|})$, because $\\frac{A}{|p_0|}>0$\n",
        "\n",
        "Now, as $\\bar{z}\\in[-1,1]$, then $\\bar{z}\\in(-1-\\frac{A}{|p_0|},1+\\frac{A}{|p_0|})$."
      ],
      "metadata": {
        "id": "4a8TQZoZpUwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will calculate the eigenvectors corresponding to the eigenvalues of the matrix $\\textbf{A}$ computed in the previous code. To do this, we are going to use the Newton's method for linear systems and find one vector of the space $Nul(\\textbf{A}-\\lambda I)$."
      ],
      "metadata": {
        "id": "4piA-JoH9Rma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Ffun(F, x):\n",
        "  lis = []\n",
        "  for y in F:\n",
        "    acum = 0\n",
        "    cont = 0\n",
        "    for z in y:\n",
        "      acum+= z*x[cont]\n",
        "      cont+=1\n",
        "    lis.append(acum)\n",
        "  return lis\n",
        "\n",
        "def JFfun(F, x):\n",
        "  lis = []\n",
        "  for y in F:\n",
        "    sublis = []\n",
        "    for z in range(len(y)):\n",
        "      cont = -1\n",
        "      acum = 0\n",
        "      for w in y:\n",
        "        cont+=1\n",
        "        if cont == z:\n",
        "          continue\n",
        "        acum += w*x[cont]\n",
        "\n",
        "      sublis.append(acum)\n",
        "    lis.append(sublis)\n",
        "  return lis\n",
        "\n",
        "def newdim(F, P, delta, epsilon, max1):\n",
        "    Y = np.array(Ffun(F,P))\n",
        "    for k in range(1, max1 + 1):\n",
        "        J = np.array(JFfun(F,P))\n",
        "        Q = P - np.linalg.solve(J, Y)\n",
        "        Z = Ffun(F,Q)\n",
        "        err = np.linalg.norm(Q - P)\n",
        "        relerr = err / (np.linalg.norm(Q) + np.finfo(float).eps)\n",
        "        P = Q\n",
        "        Y = Z\n",
        "        iter = k\n",
        "\n",
        "        if err < delta or relerr < delta or np.linalg.norm(Y) < epsilon:\n",
        "            break\n",
        "\n",
        "    return P, iter, err\n",
        "\n",
        "for x in s:\n",
        "  F = A-(np.identity(dimA))*x\n",
        "  P, iter, err = newdim(F, np.array([1,1,1]), 1e-10, 1e-10, 10)\n",
        "  print(f\"Uno de los vecores propios de la matriz asociado al valor propio {x} es: \\n{P}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "sxrBmCyrF-V6",
        "outputId": "8879a1cb-bf39-41a9-e349-96af360e7b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LinAlgError",
          "evalue": "1-dimensional array given. Array must be at least two-dimensional",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5fc43c95f151>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSymbol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mJF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambdify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5fc43c95f151>\u001b[0m in \u001b[0;36mnewdim\u001b[0;34m(F, JF, P, delta, epsilon, max1)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJFfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    393\u001b[0m     \"\"\"\n\u001b[1;32m    394\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_makearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m     \u001b[0m_assert_stacked_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m     \u001b[0m_assert_stacked_square\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_makearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_assert_stacked_2d\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             raise LinAlgError('%d-dimensional array given. Array must be '\n\u001b[0m\u001b[1;32m    207\u001b[0m                     'at least two-dimensional' % a.ndim)\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLinAlgError\u001b[0m: 1-dimensional array given. Array must be at least two-dimensional"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Error Analysis\n",
        "\n",
        "For our first step, that we used Leverrier's method, doesn't have an associated theorical error, cause it give us theoretically the exact values for every coefficient of the characteristic polynomial. Then, the error just can be with the approximations of the machine.\n",
        "\n",
        "For our second step, the error associated is just the error for Newton's method, wich, when it converges, its error is bounded by the next form:\n",
        "\n",
        "$|z-z_{n+1}|\\leq{\\frac{|f''(\\xi)|}{2|f'(z)|}}|z-z_n|^2$, where $z$ is a real such that $f(z)=0$, and $\\xi$ is a point in $[p_n,p]$ or $[p,p_n]$. In our particular case, $P''$ exists because it's a polynomial.\n",
        "\n",
        "Besides, in our case, with our program we have that:\n",
        "\n",
        "$|z_n-z_{n+1}|<10^{-10}$, where $z_{n+1}$ is our final approximation and $z_n$ the previous one.\n",
        "\n",
        "And $|P(z)|<10^{-10}$, where $z$ is our approimation. If one of these is true, then the program stops.\n",
        "\n",
        "For our final step we use again Newton's method, but this time, for linear system of equations, in our case, our way of dealing with the error is:\n",
        "\n",
        "$||X_n-X_{n+1}||<10^{-10}$, where $X_{n+1}$ is our final approximation of the eignvector.\n",
        "\n",
        "And $||f(X)||=||\\begin{pmatrix}f_1(x_1,\\cdots,x_n)\\\\\\vdots\\\\f_n(x_1,\\cdots,x_n)\\end{pmatrix}||<10^{-10}$, where $X$ is our approximation and $f_i$ is the linear fuction associated in each case. If one of these is true, then the program stops."
      ],
      "metadata": {
        "id": "yklwOL10HS09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Method Analysis\n",
        "The Power Method is an iterative approach for finding the dominant eigenvalue of a matrix\n",
        "(the eigenvalue with the largest absolute value) and its corresponding eigenvector. Starting with\n",
        "an arbitrary initial vector x0, the method repeatedly applies the matrix to this vector, eventually\n",
        "converging to the dominant eigenvector if certain conditions are met.\n",
        "### Motivation\n",
        "The initial motivation of this method is to find the spectral radius of a matrix $A$, which is defined as the largest eigenvalue associated with the matrix $A$. This value is used in many contexts, an example is the analysis of the numerical stability of many methods, such as Newton's method for functions of several variables."
      ],
      "metadata": {
        "id": "P6C8uMO74NZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Theory\n",
        "Let $A$ be a square matrix of $\\mathbb{R}^{n \\times n}$. Let $\\lambda_i$ and $v_i$ be the eigenvalues ​​and eigenvectors associated with the matrix $A$, so that $Av_i = \\lambda_i v_i \\quad \\forall \\, 1 \\leq i \\leq n$ (1) where $v_i$ and $v_j$ are linearly independent vectors $\\forall \\, i \\neq j$ (2)\n",
        "\n",
        "We want a method to calculate the dominant eigenvalue of (1). Let us initially assume that $ |\\lambda_1| > |\\lambda_2| > \\cdots > |\\lambda_n|. $\n",
        "In this way, we know that the dominant value in this case would be $\\lambda_1$. Let us then define the following sequence: let $x_0 \\in \\mathbb{R}^n$ be as close as we want to $v_1$. Then, from (2), we know from the theory of linear algebra that such eigenvectors form a basis for $\\mathbb{R}^n$, so:\n",
        "\n",
        "$\\exists \\, c_1, \\dots, c_n \\in \\mathbb{R} \\text{ such that } x_0 = \\sum _{i=1}^n c_i v_i. \\tag{1*}$\n",
        "\n",
        "Let's define: $x_{k+1} = \\frac{Ax_k}{\\|Ax_k\\|}.$ Initially we must observe that $\\|x_{k+1}\\| = 1$. (3) Applying the method of descent to the sequence, we can rewrite it as: $x_{k+1} = \\frac{1}{\\alpha_k} A^k x_0,$ where $\\alpha_k \\geq 0 \\, \\forall \\, k$. By (3), then: $1 = \\|x_{k+1}\\| = \\left\\|\\frac{1}{\\alpha_k} A^k x_0 \\right\\| = \\frac{1}{\\alpha_k} \\|A^k x_0\\|.$ Therefore, $\\alpha_k = \\|A^k x_0\\|$ and we can rewrite the sequence as: $x_{k+1} = \\frac{1}{\\|A^k x_0\\|} A^k x_0.$ Now, multiplying by $A^k$ to the right at (1*), we obtain:\n",
        "\n",
        "$A^k x_0 = \\sum _{i=1}^n c_i A^k v_i. \\tag{2*}$\n",
        "\n",
        "\n",
        "Note that, $\\forall \\, 1 \\leq i \\leq n$, $A^k v_i = \\lambda_i^k v_i$. Therefore in (2*) we can again rewrite:\n",
        "\n",
        "$x_{k+1} = \\frac{1}{\\|A^k x_0\\|} \\sum _{i=1}^n c_i \\lambda_i^k v_i. \\tag{3*}$\n",
        "\n",
        "Hence, the vectors $x_{k+1}$ are approximately parallel to $v_1$, that is, both vectors are approximately linearly dependent. If we assume that the method converges, then we can calculate the eigenvector dominat.\n",
        "\n",
        "Now since the succession must fulfill that $Ax_{x} = \\lambda_k x_{k}$,\n",
        "multiplying by the transpose of $x_k$ to the left then: $x_k^T Ax_k = x_k^T \\lambda_k x_k$ and we can rewrite this like:\n",
        "\n",
        "$\\lambda_k = \\frac{x_k^T Ax_k}{x_k^T x_k}\\tag{4*}$\n",
        "\n",
        "if we can proof that the succession $(x_k)$ converges then $x_k^T$ converges\n",
        "too, since all components of $(x_k)$ converges too. Suposse that $x_k \\to \\alpha´ v_1$, then both $Ax_k \\to A\\alpha v_1$, $x_k^T \\to \\alpha v_1^T$\n",
        "therefore at (4*)\n",
        "\n",
        "$\\lambda_k \\to \\frac{\\alpha v_1 \\alpha \\lambda_1 v_1}{\\alpha v_1^T \\alpha v_1}\\tag{5*} = \\frac{\\alpha v_1 A \\alpha v_1}{\\alpha v_1^T \\alpha v_1} = \\lambda_1$\n",
        "\n",
        "since $Av_1 = \\lambda_1 v_1$. Now we start to proof that:\n",
        "\n",
        "### Convergence\n",
        "\n",
        "By (3*) we now that: $x_{k+1} = \\frac{1}{\\|A^k x_0\\|} \\sum _{i=1}^n c_i \\lambda_i^k v_i.$. Taking out common factor $\\lambda_1^k$ we rewrite the above as:\n",
        "$x_{k+1} = \\frac{\\lambda_1^k}{\\|A^k x_0\\|} (\\sum _{i=1}^n c_i (\\frac{\\lambda_i}{\\lambda_1})^k v_i) \\tag{6*}$\n",
        "\n",
        "since $\\|x_{k+1}\\| = 1$ by definition of $(x_k)$ then at (6*)\n",
        "\n",
        "$1 = \\|x_{k+1}\\| = \\|\\frac{\\lambda_1^k}{\\|A^k x_0\\|} (\\sum _{i=1}^n c_i (\\frac{\\lambda_i}{\\lambda_1})^k v_i) \\|\\tag{7*}$\n",
        "\n",
        "where we can rewrite (7*) like:\n",
        "\n",
        "$\\|A^kx_0\\| = |\\lambda_1|^k \\|\\sum _{i=1}^n c_i (\\frac{\\lambda_i}{\\lambda_1})^k v_i\\| \\tag{8*}$\n",
        "\n",
        "now at (6*) by (8 *) we can rewrite that like:\n",
        "\n",
        "$x_{k+1} = \\frac{\\lambda_1^k}{|\\lambda_1|^k \\|\\sum _{i=1}^n c_i (\\frac{\\lambda_i}{\\lambda_1})^k v_i\\|} (\\sum _{i=1}^n c_i (\\frac{\\lambda_i}{\\lambda_1})^k v_i) \\tag{9*}$\n",
        "\n",
        "Therefore we continue to rewrite the sequence as follows, by (9*):\n",
        "\n",
        "$x_{k+1} = \\frac{\\frac{}{+} {1}}{\\|\\sum _{i=1}^n c_i (\\frac{\\lambda_i}{\\lambda_1})^k v_i\\|} (\\sum _{i=1}^n c_i (\\frac{\\lambda_i}{\\lambda_1})^k v_i) \\tag{10*}$\n",
        "\n",
        "at (10*) now when $k \\to + ∞$ then: $x_{k+1} \\to \\frac{\\frac{}{+}{1}}{\\|c_1 v_1\\|} c_1 v_1$ because the succesion $(\\frac{\\lambda_i}{\\lambda_1})^k \\to 0$ since $|\\lambda_1|>...>|\\lambda_n|$.\n",
        "\n",
        "\n",
        "Therefore the sequence $(x_k)$ converges and as we saw at (5*) the sequence $(\\lambda_k)$ must also converges to $\\lambda_1$ taking $\\alpha = \\frac{\\frac {}{+}{1}}{\\|c_1 v_1\\|}$.\n",
        "\n",
        "### Error Analysis\n",
        "\n",
        "Now that we know that the method converges, we are interested in knowing its numerical stability, that is, knowing what the convergence rate of the method is so that it is numerically stable. In other words find $c>0$ for which $|E_k| <= c|E_0|$\n",
        "\n",
        "We continues noting that for the kth iteration we have to:\n",
        "\n",
        "$|E_{k+1}| = \\|x_{k+1} - \\alpha c_1 v_1\\|$ where $|\\alpha| = |\\frac{1}{\\|c_1 v_1\\|}|$ keep doing we have:\n",
        "\n",
        "$|E_{k+1}| = \\|\\sum_{i = 1}^n c_i \\lambda_i^k v_i - \\alpha c_1 v_1\\| \\tag{11*}$\n",
        "\n",
        "By the triangular inequality we follow that, at (11*):\n",
        "\n",
        "$|E_{k+1}|<= \\sum_{i=1}^n\\|c_1 \\lambda_i^k v_1\\| + \\|\\alpha c_1 v_1\\| \\tag{12*}$\n",
        "\n",
        "but $\\|\\alpha c_1 v_1\\| = 1$ then at (12*) rewriten:\n",
        "\n",
        "$|E_{k+1}|<= \\sum_{i=1}^n|\\lambda_i^k|\\|c_1 v_1\\| + 1 \\tag{13*}$\n",
        "\n",
        "now how we can suposse at the bigening we can do at (13*):\n",
        "\n",
        "$|E_{k+1}|<= |\\lambda_1|^k\\sum_{i=1}^n\\|c_1 v_1\\| + 1 \\tag{14*}$\n",
        "\n",
        "cause $|\\lambda_1| > ... > |\\lambda_n|$, now how to the k = 0 step we have: $|E_0| = \\|x_0 - \\alpha c_1 v_1\\| = \\|\\sum_{i=1}^n c_i v_i - \\alpha c_1 v_1\\| >= \\|\\sum_{i=1}^n c_i v_i\\| - 1$ adding 1 to both members then $|E_0| + 1 >= \\|\\sum_{i=1}^n c_i v_i\\|$ and multiplying by $|\\lambda_1|^k$ member by member, then adding 1 we get $|\\lambda_1|^k \\sum_{i=1}^n (c_i v_i) + 1<|\\lambda_1|^k(|E_0|+1)+1$ and how $|E_0|>0$ then it must happen at(14*)\n",
        "\n",
        "$|E_{k+1}|<=|\\lambda_1|^k\\sum_{i=1}^n\\|c_1 v_1\\| + 1 <= |\\lambda_1|^k(|E_0|+1)+1 <= (|\\lambda_1|^k +1)|E_0|$\n",
        "\n",
        "therefore $C = |\\lambda_1|^k + 1$ and the method its numerically stable.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EPWDPe1D40kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def power_method(A, tol=1e-10, max_iter=1000):\n",
        "\n",
        "    n = A.shape[0]\n",
        "    # Initial random vector\n",
        "    v = np.random.rand(n)\n",
        "    v = v / np.linalg.norm(v)  # Initial normalization\n",
        "\n",
        "    lambda_old = 0\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        # Matrix-vector multiplication\n",
        "        w = np.dot(A, v)\n",
        "        # Normalization\n",
        "        v = w / np.linalg.norm(w)\n",
        "        # Compute the new dominant eigenvalue\n",
        "        lambda_new = np.dot(v, np.dot(A, v))\n",
        "\n",
        "        # Check for convergence\n",
        "        if abs(lambda_new - lambda_old) < tol:\n",
        "            break\n",
        "\n",
        "        lambda_old = lambda_new\n",
        "\n",
        "    return lambda_new, v\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example matrix\n",
        "    A = np.array([[4, 1],[2, 3]])\n",
        "\n",
        "    dominant_eigenvalue, eigenvector = power_method(A)\n",
        "    print(f\"Dominant eigenvalue: {dominant_eigenvalue}\")\n",
        "    print(f\"Dominant eigenvector: {eigenvector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoLvBQnlZdHu",
        "outputId": "d24a478f-5fd7-4069-9077-b4119f9b3636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dominant eigenvalue: 4.999999999937861\n",
            "Dominant eigenvector: [0.70710678 0.70710678]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QR Method\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Just like all the previous methods, our problem is to determine the eigenvalues ​​and eigenvectors of a matrix, now with the QR method we want a matrix converging towards an upper triangular shape, where the eigenvalues ​​appear on the main diagonal. In this way it is easier to find their respective values.\n",
        "\n",
        "## Definition\n",
        "\n",
        "Let $A∈ℝ^{n \\times n}$ and let $Q$ and $R$ be square matrix in $ℝ^{n \\times n}$ whith the property\n",
        "$A = QR\\tag{1*}$\n",
        "where $Q^T Q = I$ and $R$ is a upper triangular matrix We want a method that give us these matrix. So let be the succession:\n",
        "$A_{k} = Q_kR_k\\tag{2*}$\n",
        "where $Q_k^T Q_k = I$ and $R_k$ satisfies the above. And let be: $A_{k+1} = R_k Q_k$ then we can rewrite (2*) like:\n",
        "$Q_k^T Q_k A_{k+1} = A_{k+1} = Q_k^T Q_k R_k Q_k = Q_k^T A_k Q_k \\tag{3*}$\n",
        "\n",
        "## Convergence\n",
        "\n",
        "Let be $A_0$ a square matrix fiwed, and let be ${\\lambda_1,...,\\lambda_n}$ be eigenvalues ​​of matrix A that satisfies $|\\lambda_1| > |\\lambda_2| > ... >|\\lambda_n|$ first note that if we apply decent method at (3*) then we can obtain:\n",
        "\n",
        "$A_{k+1} = Q_k^TQ_{k-1}^T...Q_{1}^T A_0Q_1Q_2...Q_k \\tag{4*}$\n",
        "\n",
        "now notice that a product of transposed matrices is transposed that satisfies that $Q_k^T Q_k = I$. Let $P_k = Q_1...Q_k$ so at (4*) we can rewrite that like:\n",
        "\n",
        "$A_{k+1} = P_k^TA_0P_k \\tag{5*}$\n",
        "\n",
        "now let be $X$ the matriz that satisfies $X$ denotedenote the matrix whose ith column is a eigenvector of A corresponding to $\\lambda_i$ and\n",
        "suppose that $X^{-1}$ has an LU decomposition.\n",
        "Now under our assumptions, we can write $A = XDX^{-1}$ where D = diag$(\\lambda_1, . . . , \\lambda_n)$ and\n",
        "X is a real matrix of eigenvectors of A. We know there is a factorization of X = QR, where\n",
        "Q is orthogonal and R is upper triangular. Then\n",
        "$A = QRDR^{-1}Q$ and so $Q^{-1}AQ = RDR^{-1}\n",
        ".$\n",
        "Since $RDR^{-1}$\n",
        "is the product of upper triangular matrices, it is also upper triangular, and\n",
        "hence we know that $Q^{-1}AQ = Q^TAQ$ is upper triangular. Note that the eigenvalues of A\n",
        "will then lie of the diagonal of $Q^TAQ$.\n",
        "To finishin, notice that if $k \\to + ∞$ then $A_{k+1} = P_k^TA_0P_k \\to Q^TAQ$.\n",
        "In other words, the matrices $A_k$ are converging to an upper triangular matrix whose diagonal\n",
        "elements are the eigenvalues of A.\n",
        "(This proof is over our hands but you can find the proof: https://sites.math.rutgers.edu/~falk/math574/lecture9.pdf)"
      ],
      "metadata": {
        "id": "giRQC6XXgFAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def QR(A):\n",
        "    n, m = A.shape # get the shape of A\n",
        "\n",
        "    Q = np.empty((n, n)) # initialize matrix Q\n",
        "    u = np.empty((n, n)) # initialize matrix u\n",
        "\n",
        "    u[:, 0] = A[:, 0]\n",
        "    Q[:, 0] = u[:, 0] / np.linalg.norm(u[:, 0])\n",
        "\n",
        "    for i in range(1, n):\n",
        "\n",
        "        u[:, i] = A[:, i]\n",
        "        for j in range(i):\n",
        "            u[:, i] -= (A[:, i] @ Q[:, j]) * Q[:, j]\n",
        "\n",
        "        Q[:, i] = u[:, i] / np.linalg.norm(u[:, i])\n",
        "\n",
        "    R = np.zeros((n, m))\n",
        "    for i in range(n):\n",
        "        for j in range(i, m):\n",
        "            R[i, j] = A[:, j] @ Q[:, i]\n",
        "\n",
        "    return Q, R\n",
        "\n",
        "def valoresPropios(A, max=1000, tol=1e-10):\n",
        "    n = A.shape[0]\n",
        "    A_k = A.copy()\n",
        "\n",
        "    for x in range(max):\n",
        "\n",
        "        Q, R = QR(A_k)\n",
        "        A_next = R @ Q\n",
        "\n",
        "        if np.linalg.norm(A_next - A_k) < tol:\n",
        "            break\n",
        "\n",
        "        A_k = A_next\n",
        "\n",
        "    eigenvalues = np.diag(A_k)\n",
        "    return eigenvalues\n",
        "\n",
        "inp = input().split(\";\")\n",
        "list = []\n",
        "for x in inp:\n",
        "  list.append([int(y) for y in x.split(\",\")])\n",
        "lis = []\n",
        "lip = []\n",
        "A = np.array(list, dtype=float)\n",
        "valoresPropios(A)"
      ],
      "metadata": {
        "id": "Vxvyk0BVgSpu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}